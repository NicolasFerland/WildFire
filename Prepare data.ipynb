{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2000 tmin is not complete yet. So nyear < 8\n",
    "\n",
    "# Total size: 365*24*1405*621*5 = 38 billions. There are 4 bytes in one real, so it's 152 GB.\n",
    "# I need to divide it at least by 100 for memory purpose. I can do that by averaging on regions.\n",
    "\n",
    "nyear = 1 # Up to 24\n",
    "nleapday = 1 # Up to 6\n",
    "nx, ny = 140, 62 # Will give steps of 10 or 11\n",
    "#nx, ny = 88, 39 # Will give steps of 16 or 17\n",
    "#nx, ny = 351, 155 # Will give steps of 4 or 5.\n",
    "\n",
    "# The biggest fire is 600,000 acres = 2428 km2 = 49 km*49 km. By default, the area are 4 km*4 km.\n",
    "# I can combine region afterwards, so doing large region is useful only to reduce data.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import rasterio\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Split data into region of latitude and longitude.\n",
    "ybin = np.linspace(0,621,ny, dtype=int)\n",
    "xbin = np.linspace(0,1405,nx, dtype=int)\n",
    "ds = rasterio.open('PRISMdaily\\PRISM_ppt_stable_4kmD2_19920101_bil.bil')\n",
    "affine = ds.affine\n",
    "data = ds.read(1)\n",
    "latbin = ybin*affine[4]+affine[5]\n",
    "longbin = xbin*affine[0]+affine[2]\n",
    "loc = []\n",
    "for ix in range(nx-1):\n",
    "    for iy in range(ny-1):\n",
    "        # Verify we have a full cell.\n",
    "        inputData0 = data[ybin[iy]:ybin[iy+1],xbin[ix]:xbin[ix+1]]\n",
    "        if np.any(inputData0==-9999.):\n",
    "            continue\n",
    "        loc.append((ix, iy))\n",
    "loc = np.array(loc)\n",
    "\n",
    "# day\n",
    "nday0 = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "nday1 = [31,29,31,30,31,30,31,31,30,31,30,31]\n",
    "def nday(year,month):\n",
    "    if year%4 == 0:\n",
    "        return nday1[month-1]+1\n",
    "    else:\n",
    "        return nday0[month-1]+1\n",
    "\n",
    "# Prepare input data\n",
    "inputData = []\n",
    "for year in range(1992,1992+nyear):\n",
    "  for month in range(1,13):\n",
    "    for day in range(1,nday(year,month)):\n",
    "        ds1 = rasterio.open('PRISMdaily\\PRISM_ppt_stable_4kmD2_'+str(year)+str(month).zfill(2)+str(day).zfill(2)+'_bil.bil')\n",
    "        ds2 = rasterio.open('PRISMdaily\\PRISM_tmax_stable_4kmD1_'+str(year)+str(month).zfill(2)+str(day).zfill(2)+'_bil.bil')\n",
    "        ds3 = rasterio.open('PRISMdaily\\PRISM_tmean_stable_4kmD1_'+str(year)+str(month).zfill(2)+str(day).zfill(2)+'_bil.bil')\n",
    "        ds4 = rasterio.open('PRISMdaily\\PRISM_tmin_stable_4kmD1_'+str(year)+str(month).zfill(2)+str(day).zfill(2)+'_bil.bil')\n",
    "        data = np.array([ds1.read(1),ds2.read(1),ds3.read(1),ds4.read(1)])\n",
    "        inputDataT=[]\n",
    "        for ix, iy in loc:\n",
    "            inputDataT.append(np.mean(data[:,ybin[iy]:ybin[iy+1],xbin[ix]:xbin[ix+1]],axis=(1,2)))\n",
    "        inputData.append(inputDataT)\n",
    "inputData = np.array(inputData)\n",
    "\n",
    "# Prepare target\n",
    "conn = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('''SELECT DISCOVERY_DATE, FIRE_SIZE, LATITUDE, LONGITUDE \n",
    "FROM FIRES''')\n",
    "rows = c.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "df.columns = ['time', 'size', 'lat', 'lon']\n",
    "\n",
    "# drop Alaska, Hawaii, Puerto Rico\n",
    "# Or maybe use BoundingBox(left=-125.02083333333333, bottom=24.06249999999996, right=-66.47916666666197, top=49.93750000000203)\n",
    "df = df[df['lon']>-130.]\n",
    "df = df[df['lat']>24.]\n",
    "df = df[df['lat']<50.]\n",
    "\n",
    "target = []\n",
    "for ix, iy in loc:\n",
    "    tmp = df[ (df['lon']>longbin[ix]) &  (df['lon']<longbin[ix+1]) &  (df['lat']<latbin[iy]) &  \n",
    "             (df['lat']>latbin[iy+1]) ]\n",
    "    dataT = []\n",
    "    for it in range(2448623,2448623+365*nyear+nleapday):\n",
    "        tmp2 = tmp[(tmp['time']>it) & (tmp['time']<it+1)]\n",
    "        if tmp2.empty:\n",
    "            dataT.append(0)\n",
    "            continue\n",
    "        dataT.append(np.sum(tmp2['size'].values))\n",
    "    target.append(np.array(dataT))\n",
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4389, 366)\n",
      "(4389, 2)\n",
      "(365, 4389, 4)\n"
     ]
    }
   ],
   "source": [
    "# I made a mistake and missed february 9 for inputData\n",
    "\n",
    "print(target.shape)\n",
    "print(loc.shape)\n",
    "print(inputData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4389, 365)\n"
     ]
    }
   ],
   "source": [
    "# drop february 29 for target (I won't need that later)\n",
    "\n",
    "# february 29 is the 29+31 rows, it starts at 0, so it's 29+31-1=59\n",
    "# axis=1 since it's the time\n",
    "target = np.delete(target, (59), axis=1)\n",
    "\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4389, 365, 4)\n",
      "(4389, 365)\n"
     ]
    }
   ],
   "source": [
    "# Flip axis for inputData\n",
    "inputData = np.swapaxes(inputData,0,1)\n",
    "print(inputData.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# target is (number of xy localization, number of days)\n",
    "# inputData is (number of xy localization, number of days, 4)\n",
    "# loc is (number of xy localization,2)\n",
    "\n",
    "# I need to combine target and inputData0 in one file and save it.\n",
    "# I need to save loc since it can be useful to combine data\n",
    "\n",
    "Data = np.concatenate((inputData, target[:,:,np.newaxis]),axis=2)\n",
    "# It will be (number of xy localization, number of days, 5)\n",
    "\n",
    "np.save('Data', Data, allow_pickle=True, fix_imports=False)\n",
    "# !!!Should test it on other notebook first!!!\n",
    "\n",
    "np.save('LocalizationData', loc, allow_pickle=True, fix_imports=False)\n",
    "\n",
    "# For 365 days, 4389 xy bins, it's 62 MB\n",
    "# For 24 years, 4389 xy bins, it will be 1.5 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now need to use this data for forecasting.\n",
    "\n",
    "I could use RNN or VARIMA (or ARIMAX), though VARIMA is used when one to also use the previous state of the forecasted time series, when I only want to use the weather time series to forecast the fire one. Someone else talked about Kalman filters or state-space models (SSMs).\n",
    "\n",
    "I don't think I should remove the seasonality and the trend since it should be determined from the weather data. Some special day like July 4th can affect my results, it might be worth it to correct the fire excedent due to July 4th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
